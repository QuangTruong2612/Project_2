{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce12fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad79946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e467e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProcessedDataConfigure:\n",
    "    root_dir: Path\n",
    "    train_path: Path\n",
    "    test_path: Path\n",
    "    data_path: Path\n",
    "    vectorizer_path: Path\n",
    "    test_size: int\n",
    "    random_state: int\n",
    "    target: str\n",
    "    cols_not_use: list\n",
    "    ngram: tuple\n",
    "    max_features: int\n",
    "    min_df: int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae08d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Project_2.constants import *\n",
    "from Project_2.utils.common import read_yaml, create_directories\n",
    "from pathlib import Path\n",
    "\n",
    "class ConfigureManager:\n",
    "    def __init__(self,\n",
    "                config_filepath: Path = CONFIG_FILE_PATH,\n",
    "                params_filepath: Path = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_processed_data_config(self) -> ProcessedDataConfigure:\n",
    "        processed = self.config.data_process\n",
    "        params = self.params\n",
    "        create_directories([processed.root_dir])\n",
    "\n",
    "        processed_data_config = ProcessedDataConfigure(\n",
    "            root_dir = Path(processed.root_dir),\n",
    "            train_path = Path(processed.train_path),\n",
    "            test_path = Path(processed.test_path),\n",
    "            data_path = Path(processed.data_path),\n",
    "            vectorizer_path = Path(processed.vectorizer_path),\n",
    "            test_size = params.TEST_SIZE,\n",
    "            random_state= params.RANDOM_STATE,\n",
    "            target= params.TARGET,\n",
    "            cols_not_use = params.COLS_NOT_USE,\n",
    "            ngram = tuple(params.NGRAM),\n",
    "            max_features = params.MAX_FEATURES,\n",
    "            min_df = params.MIN_DF,\n",
    "        )\n",
    "        return processed_data_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "class ProcessedData:\n",
    "    def __init__(self, config = ProcessedDataConfigure):\n",
    "        self.config = config\n",
    "        self.stopwords = {\n",
    "                            'thì', 'là', 'mà', 'và', 'của', 'những', 'các', 'cái', 'việc', 'bị', 'bởi',\n",
    "                            'shop', 'sản_phẩm', 'hàng', 'giao', 'mua', 'bán', 'mình', 'tiki', 'shopee',\n",
    "                            'nhé', 'nha', 'ạ', 'ơi', 'nhen', 'kaka', 'hihi'\n",
    "                        }\n",
    "\n",
    "        self.teencode = {\n",
    "                        'k': 'không', 'ko': 'không', 'kh': 'không', 'hok': 'không',\n",
    "                        'dc': 'được', 'đc': 'được',\n",
    "                        'bt': 'bình thường',\n",
    "                        'wa': 'quá',\n",
    "                        'uk': 'ừ',\n",
    "                        'z': 'vậy',\n",
    "                        'sp': 'sản phẩm'\n",
    "                    }\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "                                            ngram_range=self.config.ngram,\n",
    "                                            max_features=self.config.max_features,\n",
    "                                            min_df=self.config.min_df # Bỏ qua những từ xuất hiện quá ít (dưới 2 lần)\n",
    "                                        )\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            path = self.config.data_path\n",
    "            data = pd.read_csv(path)\n",
    "            print(\"Load data completed!\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise e\n",
    "\n",
    "    # Droip columns not use in training. You can change cols in params\n",
    "    def drop_cols_not_use(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            data = data.drop(self.config.cols_not_use, axis=1)\n",
    "            print(\"Drop columns not use in training !\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise e\n",
    "\n",
    "    # clean text\n",
    "    def clean_text(self, text : str) -> None:\n",
    "        # lower\n",
    "        text = text.lower()\n",
    "\n",
    "        # remove emoji\n",
    "        emoji_pattern  = re.compile(\n",
    "                                    u\"[\\U0001F600-\\U0001F64F\"  # Emoticons (Mặt cười...)\n",
    "                                    u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                                    u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map\n",
    "                                    u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols (Các emoji mới)\n",
    "                                    u\"\\u2600-\\u26FF\"          # Misc Symbols (Dấu tim, quân bài...)\n",
    "                                    u\"\\u2700-\\u27BF\"          # Dingbats\n",
    "                                    u\"\\u200D\\uFE0F]+\"         # Zero-width joiner (cho các emoji ghép)\n",
    "                                )\n",
    "\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        # remove special text\n",
    "        text = re.sub(r'[^\\w\\s,.!?]', '', text)\n",
    "        text = re.sub(r'[.,!?;:]', '', text)\n",
    "\n",
    "        # processed teen code\n",
    "        words = text.split()\n",
    "        words = [self.teencode.get(word, word) for word in words]\n",
    "        text = ' '.join(words)\n",
    "\n",
    "        text_tokenized = ViTokenizer.tokenize(text)\n",
    "\n",
    "        tokens = text_tokenized.split()\n",
    "        clean_tokens = [t for t in tokens if t not in self.stopwords]\n",
    "\n",
    "        return ' '.join(clean_tokens)\n",
    "\n",
    "    def split_data(self, data: pd.DataFrame) -> tuple:\n",
    "        try:\n",
    "            X = data[\"content\"].tolist()\n",
    "            y = data[self.config.target].tolist()\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                                X, y,\n",
    "                                                                test_size=self.config.test_size,\n",
    "                                                                random_state=self.config.random_state,\n",
    "                                                                stratify=y\n",
    "                                                            )\n",
    "            print(\"Split data completed!\")\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def save_data(self, X_train, X_test, y_train, y_test) -> None:\n",
    "        try:\n",
    "            train_data = pd.DataFrame({ \"content\": X_train, self.config.target: y_train })\n",
    "            test_data = pd.DataFrame({ \"content\": X_test, self.config.target: y_test })\n",
    "\n",
    "            train_data.to_csv(self.config.train_path, index=False)\n",
    "            test_data.to_csv(self.config.test_path, index=False)\n",
    "\n",
    "            print(f\"Save data completed! \\nTrain path: {self.config.train_path} \\nTest path: {self.config.test_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise e\n",
    "\n",
    "        # process data\n",
    "    def process_data(self, data: pd.DataFrame) -> None:\n",
    "        try:\n",
    "            # drop cols\n",
    "            data = self.drop_cols_not_use(data)\n",
    "\n",
    "            # check null\n",
    "            print(f\"Check Null in data : {data.isnull().sum()}\")\n",
    "            data = data.dropna()\n",
    "\n",
    "            # check duplicated\n",
    "            print(f\"Check Duplicated in data : {data.duplicated().sum()}\")\n",
    "            data = data.drop_duplicates()\n",
    "\n",
    "            # clean text\n",
    "            data[\"content\"] = data[\"content\"].apply(self.clean_text)\n",
    "            print(\"Process data completed!\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = self.split_data(data)\n",
    "            self.save_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad76679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-02 01:04:34,048: INFO: common: yaml file: configs\\config.yaml loaded successfully]\n",
      "[2026-02-02 01:04:34,054: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2026-02-02 01:04:34,056: INFO: common: created directory at: artifacts]\n",
      "[2026-02-02 01:04:34,059: INFO: common: created directory at: artifacts/data_process]\n",
      "Load data completed!\n",
      "Drop columns not use in training !\n",
      "Check Null in data : rating         0\n",
      "content    18010\n",
      "dtype: int64\n",
      "Check Duplicated in data : 1768\n",
      "Process data completed!\n",
      "Split data completed!\n",
      "Save data completed! \n",
      "Train path: artifacts\\data_process\\train.csv \n",
      "Test path: artifacts\\data_process\\test.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_manager = ConfigureManager()\n",
    "    processed_data_config = config_manager.get_processed_data_config()\n",
    "\n",
    "    processed_data = ProcessedData(config=processed_data_config)\n",
    "    data = processed_data.load_data()\n",
    "    processed_data.process_data(data)\n",
    "except Exception as e:\n",
    "    print(f\"Error in stage_01_processed_data: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
